# See https://www.robotstxt.org/robotstxt.html for documentation on how to use the robots.txt file
User-agent: *
Disallow: /checkout
Disallow: /cart
Disallow: /orders
Disallow: /user
Disallow: /account
Disallow: /api
Disallow: /password
Disallow: /api_tokens
Disallow: /cart_link
Disallow: /account_link
Disallow: /cdn-cgi/

User-Agent: MJ12bot
Disallow: /

User-agent: Baiduspider
Disallow: /

User-agent: 360Spider
Disallow: /

User-agent: Yisouspider
Disallow: /

User-agent: PetalBot
Disallow: /

User-agent: Bytespider
Disallow: /

User-agent: Sogou web spider
Disallow: /

User-agent: Sogou inst spider
Disallow: /

User-agent: Amazonbot               # Amazon's user agent
Disallow: /                 # disallow this directory
Crawl-delay: 5

User-agent: SemrushBot
Crawl-delay: 5

User-agent: SiteAuditBot
Crawl-delay: 5

User-agent: bingbot
Crawl-delay: 5

User-agent: msnbot
Crawl-delay: 5
